- maybe interesting things can be learn from more in depth book theory of web scraping
- (2025-05-03) I'm stating to read the web scraping book of O'Reilly publishing (2024), chapters 1 to 3, maybe i can see some benefit in this book
- (2025-05-17) continue the reading from page 68
-(2025-05-18) beginning with the lecture of chapter 4 we manage the virtual environment with poetry for manage dependencies, install ipython and jupyter notebook for code development. The first part of this chapter the author use urllib build-in python library but we change and refactor the code with copilot chatbot for use the well known and ease of use 'requests' library. From past learning we modify the headers of the request in the parameter 'User-Agent', like I say in past project we learn that if we don't change this, the library send a generic user agent and some websites block the request
-(2025-05-18) completing the reading of the chapter 4 we learn about bs4 library for BeautifulSoup classes, this facilitates the parse and cleanning fo the html files in the web. We learn about handling http errors, connection errors and no existing tags or 'elements' in the hmtl file requested and how this ideas are important in the architecture of web scraping projects. When the project is best suited for handling exceptions and it is readable is best for the overall project, another core idea in this chapter is the importance of the reuse of code with the application of functions or OOP. Continue the reading from page 84
-(2025-05-18) in chapter 5 we learn about different strategies for parsing website, the advance ones. Learn about various of the arguments that can take the function find_all(). The idea of call the get_text() function in the last step of the algorithm, calling it before can complicate the find of elements and its contents. See how to access data through children's, descendants, siblings and parents method in bs4, this come handy when is difficult access to tags so we can go with the parser as far as the parent of the content and then navigate from here. We learn the idea for be specific with tag selection from 'landmarks' for make the web scraping project more robust to changes. We develop the power of regular expressions, that are strings that follow some rules, and his use with bs4 for access to data in the html. See how to take the content of the atributes with the .attr['name_attrs'] method for links. Extend the power of regex with lambda functions, were we use the lambda functions for prove an conditions over the tags inside the find_all() function. Finally this chapter summaries the strategies for access in the best posible manner to content through the html file. Interesting between this strategies we see in some web, the nice format of data from JavaScript files. Continue the reading from chapter 6 in page 113
-(2025-05-18) in chapter 6 we see the idea of crawlers, in previous chapter we only scrape one page or one html file, here we pass through different ones. This can be done taking all the <a> tags and his attributes 'href' on the other hand for this purpose is necessary to have in mind the limit in recursive steps that python can handle (somethin like 1000 iterations). If we need specifics links and not all the links available we can use the power of patterns in regular expression with the 're' library. A cross all the chapter is made clear the importance of handling exceptions and avoiding pitfalls in the crawler algorithm with try>except syntax. Other interesting ideas is to save the links visited in a set object for avoid scrape duplicate data. Less important comments were about dark and deep web, request library needed explicit code for handling redirections and the use of flow charts for avoid frustration before code the web crawler algorithm. One of the most interesting ideas that keep in my mind is about legal actions that can be take for crawl, this need more investigation. Continue the reading from chapter 7, page 133
- (2025-05-25) starting the chapter 7 entitled 'web model crawlers' we see the ideal planning stage for construct flexible crawlers, for that reason we use the power of OOP and his characteristics like abstraction, the book is not mentioning but we se the important of type hinting and pydantic module for manage data structure and type validation. This strategy translates into class types that takes in example dictionary to define the features of the data extracted and define tags to extract with bs4 in the target html. In the same manner defining a class for the content allows robust data models, maintainability and scalability. This strategy look interesting but have its shortcomings (more work). Remember always taking account for exceptions in the code
- (2025-05-25) in the second section of the chapter 7 we enhance the idea of web crawlers that can take topic and url for extract the content with some features. This section center in the idea of crawl website from the search path, take a topic an append to it. Other ideas view here is the strategy in the loops for crawling, it is a good idea that these crawlers avoid overloading a single web server. Like calling loops that take some topic first and scrape content about it from each website first and then pass to the other topic for the same list of websites
- (2025-05-25) next sections take a look for the model of crawlers that take content from internal link of a web. For this reason it is not necessary pass a topic and a search path of the website, just passing a specific instruction for the location of the 'href' pages we want to select is a good idea. We can use here the regular expression rule for define the target URL. One new idea that we see in this section in the logic for have the absoluteUrl parameter for the Website object, this is necessary for the crawler to know if he needs to construct the complete URL or if can go ahead and scrape the full URL given. THIS MODEL OF CRAWLER
- (2025-05-25) in the last section of the chapter 7 we see that previous sections create the most abstract objects like Website() class that do not take the differences between type of pages, like product pages and article pages. For this reason the chapter introduce the idea of subclasses and the ideas of OOP, inheritance. Where we can define child's of the Website() class and inside of these create new attributes and behaviors depending on the need. In conclusion the chapter 7 introduce basic model crawlers that are interesting but maybe need more in depth logic (is a nice place to start). Continue the reading from chapter 8 in page 155 (maybe not is about Scrapy)
- (2025-05-26) we start the reading on the chapter 9 entitled 'storage data' we see some algorithms and logic for download files form a web, we see to the characteristics that need to be given for download files of the web and what for just save the URL of a file. We see, like in other chapters, the code is developed with the urllib library and we need refactor this code with the requests library. See an introduction to storage data to a CSV and to a DBMS like MySql, here we remember that this DBMS have disadvantages and that our research expose to us the best tech in Data Base Management System, PostgreSQL (postgrez-q-l). We try to understand the book entitled 'PostgreSQL administration cookbook' from packt publishing but we lack from basic knowledge of PostgreSQL, we remember about the documentation and download this for the sql learning path, we downloading the 16.9 version maybe is better the last version 17. Begin the reading on the PostgreSQL documentation and continue web scraping book from page 181 for learn how to store data to MySql and refactor for PostgreSQL
=======
- (2025-05-03) i'm stating to read the web scraping book of OReilly publishing (2024), chapters 1 to 3, maybe i can see some benefit in this book
- (2025-05-17) conttinue the reading from page 68
-(2025-05-18) beginning with the lecture of chapter 4 we manage the virtual environment with poetry for manage dependencies, intall ipython and jupyter notebook for code development. The firs part of this cahpter the author use urllib buildin python library but we change and refactor the code with copilot chatbot for use the well known and ease of use requests library. For past learning we modify the headers of the request in the parameter 'User-Agent', like I say in past project we learn that if we dont change this, the library send a generic user agent and some websites block the request
-(2025-05-18) completing the reading of the chapter 4 we learn about bs4 library for BeautifulSoup classes, this facilitates the parese and cleanning fo the html files in the web. We learn about handling http errors, connection errors and no existing tags or 'elements' in the hmtl file requested and how this ideas are important in the architecture of web scraping projects. When the project is best suited for hadling exceptions and it is readable is best for the overall project, another core idea in this chapter is the importance of the reuse of code with the aplication of functions or OOP. Continue the reading from page 84
-(2025-05-18) in chapter 5 we learn about different strategies for parsing website, the advance ones. Learn about various of the arguments that can take the function find_all(). The idea of call the get_text() function in the last step of the algorithm, calling it before can complicate the find of elements and its contents. See to how to access data through childrens, descendants, siblings and parentsmethod in bs4, this come handy when is dificult access to tags so we can go with the parser as far as the parent of the content and then navigate from here. We learn the idea for be specific with tag selection from 'landmarks' for make the web scraping project more robust to changes. We develop the power of regular expressions, that are strings that follow some rules, and his use with bs4 for access to data in the html. See how to take the content of the atributes with the .attr['name_attrs'] method for links. Extend the power of regex with lambda functions, were we use the lambda functions for prove an conditions over the tags inside the find_all() function. Finally this chapter summaries the strategies for access in the best posible manner to content through the html file. Interesting between this strategies we see what suprise to us in some web, the nice format from JavaScript files with data. Continue the reading from chapter 6 in page 113
-(2025-05-18) in chapter 6 we see the idea of crawlers, in previous chapter we only scrape one page or one html file, here we pass through differents ones. This can be done taking all the <a> tags and his attributes 'href' on the other hand for this purpose is necessary to have in mind the limit in recursive steps that python can handle (somethin like 1000 iterations). If we need especifics links and not all the links available we can use the power of patterns in regular expression with the 're' library. A cross all the chapter is made clear the importance of handling exceptions and avoiding pitfalls in the crawler algorithm with try>except syntax. Other interesting ideas is to save the links visited in a set object for avoid scrape duplicate data. Less important comments were about dark and deep web, request library needed explicit code for handling redirections and the use of flow charts for avoid frustration befor code the web crawler algorithm. One of the most interesting ideas that keep in my mind is about legal actions that can be take for crawl, this need more investigation. Continue the reading from chapter 7, page 133
- (2025-05-25) starting the chapter 7 how title is 'web model crawlers' we see the ideal planning stage for construct flexible crawlers, for that reaso we use the power of OOP and his characteristics like abstraction, the book is not mentioning but we se the important of type hinting and pydantic module for manage data structure and type validation. This strategy translates into class types that takes in example dictionary to define the features of the data extracted and for parameters for define tags to extract with bs4 in the target html. In the same manner defining objects for the content allows robust data models, mainteinability and scalability. This strategy look interesting but have its shortcomings. Remember always taking account for exceptions in the code
- (2025-05-25) in the second section of the chapter 7 we enhance the idea of web crawlers that can take topic and url for extract the content with some features. This section center in the idea of crawl website from the search path, take a topic an append to it. Other ideas view here is the strategy for the loops for crawling, it is a good idea that these crawl avoid overloading a single web server. Like calling loops that take some topic first and scrape content about it from each website first and then pass to the other topic for the same list of websites
- (2025-05-25) next sections take a look for the model of crawlers that take content from internal link o a web. For this reason it is not necessary pass a topic and a search path of the website, just passing a specific instruction for the location of the pages we want to select is a good idea. We can use here the regular expresion rule for define the target url. One new idea that we see in this section in the logic for have the absoluteUrl parameter for the Website object, this is necessary for the crawler to know if he needs to construct the complete url of if can go ahead and scrape the full url given. THIS MODEL OF CRAWLER
- (2025-05-25) in the last section of the chapter 7 we see that previous ones create the most abstract objects like Website() class that do not take the differences between type of pages, like product pages and article pages. For this reason introduce the idea of subclasses and the attribute of OOP inheritance. Where we can define childs of the Website() class and inside of these create new attributes and behevaviors depending on the need. In conclusion the chapter 7 introduce basic model crawlers that are interesting but may be need more in depth logic (is a nice spot to start). Continue the reading from chapter 8 in page 155 (maybe not is about Scrapy)
