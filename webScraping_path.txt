- maybe interesting things can be learn from more in depth book theory of web scraping
- (2025-05-03) i'm stating to read the web scraping book of OReilly publishing (2024), chapters 1 to 3, maybe i can see some benefit in this book
- (2025-05-17) conttinue the reading from page 68
-(2025-05-18) beginning with the lecture of chapter 4 we manage the virtual environment with poetry for manage dependencies, intall ipython and jupyter notebook for code development. The firs part of this cahpter the author use urllib buildin python library but we change and refactor the code with copilot chatbot for use the well known and ease of use requests library. For past learning we modify the headers of the request in the parameter 'User-Agent', like I say in past project we learn that if we dont change this, the library send a generic user agent and some websites block the request
-(2025-05-18) completing the reading of the chapter 4 we learn about bs4 library for BeautifulSoup classes, this facilitates the parese and cleanning fo the html files in the web. We learn about handling http errors, connection errors and no existing tags or 'elements' in the hmtl file requested and how this ideas are important in the architecture of web scraping projects. When the project is best suited for hadling exceptions and it is readable is best for the overall project, another core idea in this chapter is the importance of the reuse of code with the aplication of functions or OOP. Continue the reading from page 84
-(2025-05-18) in chapter 5 we learn about different strategies for parsing website, the advance ones. Learn about various of the arguments that can take the function find_all(). The idea of call the get_text() function in the last step of the algorithm, calling it before can complicate the find of elements and its contents. See to how to access data through childrens, descendants, siblings and parentsmethod in bs4, this come handy when is dificult access to tags so we can go with the parser as far as the parent of the content and then navigate from here. We learn the idea for be specific with tag selection from 'landmarks' for make the web scraping project more robust to changes. We develop the power of regular expressions, that are strings that follow some rules, and his use with bs4 for access to data in the html. See how to take the content of the atributes with the .attr['name_attrs'] method for links. Extend the power of regex with lambda functions, were we use the lambda functions for prove an conditions over the tags inside the find_all() function. Finally this chapter summaries the strategies for access in the best posible manner to content through the html file. Interesting between this strategies we see what suprise to us in some web, the nice format from JavaScript files with data. Continue the reading from chapter 6 in page 113
-(2025-05-18) in chapter 6 we see the idea of crawlers, in previous chapter we only scrape one page or one html file, here we pass through differents ones. This can be done taking all the <a> tags and his attributes 'href' on the other hand for this purpose is necessary to have in mind the limit in recursive steps that python can handle (somethin like 1000 iterations). If we need especifics links and not all the links available we can use the power of patterns in regular expression with the 're' library. A cross all the chapter is made clear the importance of handling exceptions and avoiding pitfalls in the crawler algorithm with try>except syntax. Other interesting ideas is to save the links visited in a set object for avoid scrape duplicate data. Less important comments were about dark and deep web, request library needed explicit code for handling redirections and the use of flow charts for avoid frustration befor code the web crawler algorithm. One of the most interesting ideas that keep in my mind is about legal actions that can be take for crawl, this need more investigation. Continue the reading from chapter 7, page 133
