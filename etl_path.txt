- through the development of the learning path we see that some decisions of what to learn were accurate, the book on ETL talk about version control, oop, sql
- we are using the book with the title building etl pipelines with python from packt publishing (2023)
- (20255-05-24) we restart the lecture on this book, in preceding days we only read the intro. Now we jump to the first chapter were we see the python concepts that we gonna use through the book and projects. Most of all is a introduction to python, everything that is developed here we know. Like python functions, python OOP concepts, working with files with python, python data structures (sets, list, dict, etc.), if else conditions. Then we pass to environment managers like poetry and dotenv knowing as MMS module management systems, IDE's like pycharm and VScode and documenting dependencies. Continue the reading from page 58, chapter 2
- (2025-05-24) starting chapter 2 we see the definitions of data pipeline, this is a series of task over datas sources before outputting into some target, like storage target or function target. 'laymans term' (who was laymans?) data pipeline gets data from the source to the target. Then we pass to how to create robust data pipelines, for that we need to design a architectural plan, defining the types of data that we need to gather to the methodologies used to transform an analyze the data, in simple words we need clearly define expectations. We need to think in scalability to with an architectural design that allows handle increasing volumes of data, interesting the book for this reason talk about distributed system. Key idea for robust data pipelines is clarity in the documentation for his use.
- (2025-05-24) this chapter put a eye over the planning step of the ETL pipeline with sketch of workflow and decision over python packages and algorithm logic for ensure practices like DontRepeatYourself DRY. The explain that us like architectures of the data pipeline we need to know the needed for connect all the resources of our pipeline. For the last and for a concise data pipeline we need to put all together in a diagram project
- (2025-05-24) in the first section of the ch 2 the book talk about data pipelines in the next section we see about 'ETL' pipelines. This is the process for extract data form source, then transform and format in a specific way and finally loaded into a final storage location. The final result it is use for performing analysis and model creation. We see key differences between ETL and ELT pipelines and recommendations for the implementation considering the next aspects: large data volume ELT is better, data transformation complexity ETL is better ensure consistency before loading. If we need real time rendering ELT is best suited data extracted can be quickly loaded, if storage and compute power is a constraint then ETL is best suited, if we have power resources ELT is best. If the data comes from various sources ETL facilitates standardization before loading, for ELT we need numbers of intermediate storage like numbers of sources.
- (2025-05-24) The following section develops the three main types of ETL pipelines: i) batch processing, is used for process large volume of data and for task that not need to render results instantly like calculate the statistics from the data of sales in a e-commerce. ii) streaming processing, for real time data processing like the process of transaction in the e-commerce. iii) cloud native processing for leverage all the power of cloud services
- (2025-05-24) in the last section of the ch 2 we see how to and the benefits of automating ETL pipelines. Allows the democratization of data 'Data democratization', make easier for users to access and use the data. Make all the process more Robust, can be easily configured to handle any changes. Can free up team to focus on more important tasks. Allows the schema management, for the same benefit that can manage changes easily. Continue the reading from page 78, where chapter 3 begins
- (2025-05-24) starting the chapter 3, in the first section we see the main architectures for ETL pipelines, first start with the more simples one. This simple architecture present problems when present connection issues in the extracting phase and compromising the data extracted in sum of that we need to rerun the process and incur in new cost of computing. For thar reason the engineering data team create the ELT-P architecture that adds and PSA 'persistent staging area' this new phase ensure data availability before transform the data, this new arch is more robust but present the issue with different timing in data updates for different sources, for that the engineering data team create the ETL-VP architecture where add a VSA layer 'Volatile Staging Area' before the persistan staging area, this VSA works like a buffer it means collect all the data from different timing updates first and then in fixed intervals send a batch to the PSA. This last architecture present problems when processing task are needed in different times. Here enters the ETL Two-Phase architecture, its decoupled phases of the ETL like allows the VSA act like a buffer and pass to PSA and then in different phases run the processes needed for each computation report or output
- (2025-05-24) in the last part of the chapter 3 we setup the environment for future works we see that we gonna use the typically package for data wrangling like Pandas and Numpy but we see some new and interesting packages like Dask for distributed execution (i think for run task in parallel in different cores of the pc processor) and Numba for just in time compilation of python computations code this optimize numerical processes. We now create the folder project with poetry dependency management and packing tool and add to this environment this packages. Continue the reading from page 94 where start the part 2 of the book
- (2025-05-24) in the part 2 of the book we start with the chapter 4 and we learn about sourcing data and data extraction in simple ways really nothing to scalable, like the foundations for extract part of ETL pipelines. The chapter develop the type of existing data, structured data (like rows and columns) semi-structured data (like json files with many hirarchies) and unstructured data (like images or raw html of webs). Develop the idea of type of data sources, meaning the types of available files of data, like csv/excel, parquet files, APIs, Relational Databases/Non Relational Databases and HTML files (or webs)
- (2025-05-24) following these concepts the ch 4 develop the logic for extract data from this types of data sources. Here we learn of new requirements and methods for manage this types of files. We know Pandas for CSV. We do not know about the needed of pyarrow package for pandas to manage parquet files. We learn new stuff over API and urllib3 package but we see for manage security this package need manual setup when requests package dont need that, in this part we review concepts of http responses or status and we use the json package for manage json files (this need some investigation). On the other hand we see how urllib3 manage efficiently the request with PoolManager for that requests package use Session() Class. We review some concepts for the build-in python package for SQL, sqlite3. Last but not least we discovered some interesting method for pandas, read_html() can extrac tables form HTML files, but for that need the lxml package. In the last part of this chapter introduce the idea of logging for debbugin that is, ceeate a .log file containing the logs of the extract run, for make possible this is neede to complete some steps (review this idea, is interesting cause in the first run we encounter errors and this file help us to debbug). For conclusion i say all this was to basic i think that the extract logic gonna take data from live sources but this logic need that we have the file sources in some directory. Continue the reading from page 109 where start chapter 5
